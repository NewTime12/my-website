<!DOCTYPE html>
<html>
<head>
<title>project</title>
<link rel="stylesheet" href="213.css">
</head>
<body>
<div class="container">
  <div class="main">
    <div class="navbar">
	<div class="heading">
	 <h1 class="v">Research</h1>
	 </div>
     <div class="nav">
     <ul>
     <li><a href="project main.html">Home</a></li>
	<li><a href="people.html">People</a></li>
	<li><a href="reserch1.html">Research</a></li>
	<li><a href="publication1.html">publication</a></li>
	<li><a href="demos.html">Demos</a></li>
	
	<li><a href="software.html">Software</a></li>
	<li><a href="pratical.html">Praticals</a></li>
	<li><a href="project.html">Projects</a></li>
	<li><a href="internal.html">Internal</a></li>
	<li><a href="jobs.html">jobs</a></li>
	</ul>	 
      </div>
    </div>

     <div class="j">
	 <p class="p">Click on a dataset category to expand/collapse it.
	 Click here to expand ALL categories. Click here to collapse ALL categories.</p>
	 
	 </div>
	 <div class="i">
	  <h3 class="mk"><a href="">Sign Language Recognition</a></h3>
	 </div>
	 <div class="kk">
	 <table>
	 <tr>
	 <th class="true"> <img class="imm" src="bslalign_thumbnail1.jpg" alt="image"></th>
	 <th><h3>Aligning Subtitles in Sign Language Videos</h3></th>
	
	 </tr>
	 <tr class="ll">
	 <td></td>
	 <td>
Aligning Subtitles in Sign Language Videos
We propose a Transformer architecture to temporally align asynchronous subtitles in sign language videos.</td>
	 </tr>
	 </table>
	 <div class="kp">
	 <table>
	 <tr>
	 <th><img class="im" src="bslattend (1).jpg" alt="image"></th>
	 <th><h3>Read and Attend: Temporal Localisation in Sign Language Videos<h3></th>
	 </tr>
	 <tr class="ll">
	 <td></td>
	 
	<td>We show that the ability to localise signs emerges from the attention patterns of the Transformer sequence prediction model.</td>
	</tr>
	 
	 </table>
	</div>
	    </div>
	     </div>
	 
			   
	 
 </div>



</body>
</html>

